\chapter{Protein Location Relation Extraction: Materials \& Methods}\label{chapter:methods}
\newcommand*{\xml}[1]{\texttt{<#1>}}
\section{Task Definition}

%TODO Refer to the actual section of relatione extraction
This chapter explains the method developed to extract protein-location relations from biomedical text. This task of relation extraction comes under the broad category of natural language processing (NLP) as explained in the section \ref{sec:RE}. The task of extraction of Protein-Location relations can be formally described as follows:

Given a piece of text that contains two entity mentions, viz. protein and location, the task of relation extraction method is to decide if the text contains enough evidence for semantic relationship between these two entities. Formally, let $r=(t,arg_1,arg_2)$ denote a relationship instance where $t$ is a piece of text, $arg_1$ and $arg_2$  are entity mentions contained in text $t$, and $arg_1$ precedes $arg_2$ in the text. Given a set of relations $\left\lbrace r_i \right\rbrace$ such that every relation instance has label $l \in \left\lbrace-1,1\right\rbrace$, our task is to learn a function that can predict the label $l$ for new relation instance whose label is not known. Note that $t$ can simply be a piece of text or it may also contain structured information such as parse tree and dependency graphs. For entity mentions $arg_1$ and $arg_2$, either of them can represent a protein or a location entity.
 
% Following section to be deleted, to be shifted to introduction
\section{Replication of the state of the art in BioNLP'09}

% where should I put that
Section \ref{sec:JariBioNLP} describes the method developed by Björne et al. for the task of event extraction in BioNLP 2009. The method developed by Björne et al. along with several other methods for various tasks is available online at \url{http://jbjorne.github.io/TEES/}. I used the package TEES (Turku Event Extraction System) to get started and tried to replicate the results of his model for the task of event extraction. The selection of features in my model is influenced by the features used for the task of event extraction in TEES.

%\section{Method for protein-location relation extraction}
\subsection*{Outline of the chapter}

I will describe the method that I developed for the extraction of protein-location relations in subsequent sections. Here is the overview of sections in this chapter: 

Section \ref{sec:ssModeldsModel} explains the motivation for creating more than 1 machine learning model. Section \ref{sec:pipeline} outlines the flow of information in this task of relation extraction. Section \ref{sec:models} introduces the machine learning models. Section \ref{sec:graphRep} explains the actual representation of data in the model. The process of feature extraction and feature selection is explained in section \ref{sec:featExp} and \ref{sec:featSel} respectively.

The training methodologies and procedures are explained in section \ref{sec:training}. Section \ref{sec:experiments} talks about all different experiments tried and their effect on the overall method. Finally, section \ref{sec:tools} concludes with describing the tools used in the task.

\section{Motivation for creating multiple machine learning models}\label{sec:ssModeldsModel}

The protein-location relation is a semantic relation between a protein entity and a location entity present in the text. As explained in the section \ref{sec:corpusStats}, the protein-location relations are categorized into two key categories, viz. same sentence relations and different sentence relations. Same sentence relations are the relations in which both protein and location entity are present in the same sentence. Different sentence relations are the ones in which either of the participating entities lie in different sentence.

As discussed in the section \ref{sec:corpusStats}, around 63.81\% of total protein-location relations are same sentence relations while the remaining 36.19\% are different sentence relations. A sentence can be considered as a unit of text where grammatical rules are strictly followed. The rules that define the relationship between entities in the same sentence can be different from the rules that define the relationship between entities in the different sentence. In other words, the feature set for extracting same sentence relations could be different from the one used for extracting different sentence relations. Combining both feature sets can possibly decrease the performance of both the tasks, viz. same sentence protein-location relation extraction and different sentence protein-location relation extraction. Therefore, I decided to develop two machine learning models, one of which would be dedicated to extracting same sentence relations and the other one would be dedicated for extracting different sentence relations.

Furthermore, the semantic relationship between protein and location in the case of different sentence relations with large sentence distances might be difficult to extract due to intermittent noise and hence, I tried to focus only on those different sentence relations which have a sentence distance of 1, i.e., the protein and location entities are found in neighboring sentence.

The same sentence model designed for extracting same sentence relations would be  abbreviated as \textit{SSModel} hereafter for the purpose of brevity. The different sentence model designed for extracting different sentence relations would be abbreviated as \textit{DSModel} for the same purpose.

\section{Method pipeline}\label{sec:pipeline}

%- Explain the whole pipeline 
%- Corpus-->Sentence Segmentation-->Tokenization-->Syntactic & dependency Parsing-->Graph Representation-->  Feature Extraction --> ML Model training -->Classification on Test
%- Put a nice picture explaining the pipeline

The overall relation extraction method consists of following major stages:

\begin{enumerate}

\item \textbf{Data collection \& Corpus creation}

For LocText corpus, the annotated data is present in the JSON files that can be downloaded through the tagtog web-interface \cite{cejuela2014tagtog}. However, the JSON files only contains the text of the annotations. The whole document text is present in a separate set of HTML files. Every JSON or HTML file corresponds to a particular MEDLINE \cite{medline} document. To avoid reading from the plain files repeatedly, I decided to create a XML file containing all the corpus data along with annotations and structural details like parse tree, dependency graphs, etc.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/Corpus_Creation.png}
\caption{The process of creating a XML corpus file}\label{fig:corpusCreation}
\end{figure}

Figure \ref{fig:corpusCreation} outlines the process  of corpus creation. The data read from the HTML files is parsed using Stanford CoreNLP pipeline \cite{manning2014stanford}. In every HTML file, the data is divided into title and abstract. The text from HTML files which consists of title and abstract is initially segmented into sentences (\textit{Sentence Splitting}). Every sentences is then split into tokens (\textit{Tokenization}). Part of the speech tags are found out for every token (\textit{POS Tagging}). Finally, the dependency relations between the tokens in the sentence are computed (\textit{Dependency Parsing}). These stages of processing are explained in detail in the section \ref{sec:NLPPipeline}.

All the unstructured (plain text) and structured (sentences, tokens, POS tags, dependencies) information is then combined with annotations present in JSON files and written to the XML corpus file.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/XMLSchema1.png}
\caption{Coarse XML schema}\label{fig:XMLSchema1}
\end{figure}

The coarse XML schema of the corpus XML file can be shown in the fig. \ref{fig:XMLSchema1}. Every document (corresponds to a MEDLINE document) has two sections, viz. title and abstract. Both sections have similar structure of internal data representation denoted by \texttt{DATA FORMAT}. The data representation format \texttt{DATA FORMAT} is elaborated in fig. \ref{fig:XMLSchema2}.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/XMLSchema2.png}
\caption{Data format common to title and abstract node}\label{fig:XMLSchema2}
\end{figure}

The \texttt{DATA FORMAT} shown in fig. \ref{fig:XMLSchema2} represents the XML format used to store the data inside the title and abstract node. The nested nodes are displayed with a tab indentation. The figure is representative and does not repeat the structures of nodes for similar parent nodes. For example, even though only the node \texttt{sentence s1} shows the structure of internal nodes like tokens, essentially all the sentence nodes \texttt{sentence s2..sN} have same structure of the internal nodes.

The nodes in the \texttt{DATA FORMAT} can be explained as follows:

\begin{itemize}
\item \texttt{sentences}: This node represents collection of sentences.
  \begin{itemize}
  \item \texttt{sentence}: a basic sentence unit, containing information such as sentence text, start offset, end offset, etc.
    \begin{itemize}
	\item \texttt{tokens}: collection of tokens
	  \begin{itemize}	  
	  \item \texttt{token}	: a basic token unit containing information such as text of token, start offset, end offset, POS tag, etc.		
	  \end{itemize}
	\item \texttt{parse}: a syntactic parse tree of the sentence
	\item \texttt{dependencies}: collection of dependencies between the tokens	  
	  \begin{itemize}
	  \item \texttt{dependency}: a dependency relation containing information such as from token, to token, dependency type, etc.
	  \end{itemize}	  
	\end{itemize}	
  \end{itemize}
  \item \texttt{coreferences}:
    This node is a collection of coreferences present in the entire text unit (title or abstract)
    \begin{itemize}
    \item \texttt{coreference}: a collection of coreferent mentions
    \begin{itemize}
      \item \texttt{mention}: it contains information about the text of the coreferent mention, start token in the sentence, end token and whether this is a representative mention or not.
    \end{itemize}
    \end{itemize}

\item \texttt{proteins}: protein entity annotations
\item \texttt{organisms}: organism entity annotations
\item \texttt{locations}: subcellular location entity annotations
\item \texttt{PORelations}: protein-organism relation annotations
\item \texttt{PLRelations}: protein-location relation annotations
\end{itemize}

\item \textbf{Training \& evaluating \textit{SSModel}}

This is the second step in the relation extraction method. This step focuses solely on extracting same sentence protein-location relations from the text. 

The data is read from the corpus XML file and stored in internal data structures in the main memory. The data structures used are explained in the section \ref{sec:dataStructure}. Reading the file once also saves unnecessary I/O time and improves the execution time by a small margin. 

Since this step tries to extract the protein-location relations present in the same sentence only, every sentence in the document is analyzed one by one. In every sentence, potential protein-location relations are found out and the features are written for each one of them along with the target label. A relation is considered a potential relation if both the protein entity and the location entity are present in the same sentence. The potential relations that are present in the manual annotations are considered true relations and are denoted by target label $1$. Similarly, the potential relations that are not found in the manual annotations are considered negative relations and are denoted by target label $-1$. The support vector machine library $SVM^{light}$ \cite{joachims1999making} is used for the purpose of training and classification. Every relation is written as a feature vector. The data representation for $SVM^{light}$ is discussed in more detail in the section \ref{sec:training}.

A feature file is created each for training set, development set and test set. These files are then used by $SVM^{light}$ to train the classifiers. A model is learned from the training data. This model classifies the examples in the development set and test set. The development set is used for the tuning of the hyperparameters and the data in the test set is then finally classified. The process of training and cross-validation is explained in detail in the section \ref{sec:training}. 

The results of the classification are then evaluated. Various evaluation criteria used for the evaluation of results are discussed in section \ref{sec:evaluationCriteria}. The results of the \textit{SSModel} are also cached to be used during \textit{DSModel} evaluation and combined evaluation.

\item \textbf{Training \& evaluating different sentence model}

This step aims to extract the different sentence relations. As discussed in the section \ref{sec:ssModeldsModel}, the model is trained for extracting the different sentence relations only with the sentence distance of 1, i.e., the protein and the location entities are in the neighboring sentences. As done during the training and evaluation of \textit{SSModel}, the data is read from the corpus XML file once and stored in the internal data structures.

For every document, the sentences are processed in pairs and every pair consists of neighboring sentences. The potential different sentence protein-location relations are found out and the relations are written to the SVM feature file. A potential protein-location relation is the one which has participating entities in  neighboring sentences. For example, if sentences \texttt{s1} and \texttt{s2} are being processed, then the protein entities in \texttt{s1} can have a potential PL relationship with location entities in \texttt{s2} and vice versa.

Features are extracted for all potential relations and are written to the feature file along with the target label. Some of the features are also extracted based on the predictions of \texttt{SSModel}. As done in the training and evaluation of \texttt{SSModel}, a feature file is created each for the training set, development set and test set. 

The results of the classification are evaluated using various evaluation criteria discussed in section \ref{sec:evaluationCriteria} and the results are also cached for combined evaluation.

\item \textbf{Combined evaluation}

This is the last step in the pipeline of relation extraction method. This step primarily combines the predictions of \texttt{SSModel} and \texttt{DSModel}. The  combined predictions are uniquely evaluated with original target labels and the result of unique evaluation is calculated.

\end{enumerate}

\section{Graph representation} \label{sec:graphRep}
% Details about graph representation along with nice explanatory picture

Although the data read from the corpus XML file is stored in the internal data structures, the data is also used in the form of a graph for the purpose of feature extraction. A sentence is the most basic unit of the data which is represented as a graph. The tokens in the sentence form the nodes of the graph and the dependency relations between the tokens form the edges in the graph. The models \textit{SSModel} and \textit{DSModel} uses different representation of graphs which is explained in the following subsections.

\subsection{Graph representation for \textit{SSModel}}\label{sec:graphSSModel}

%TODO change figure
\begin{figure}
\centering
\includegraphics[scale=0.3]{figures/SameSentenceGraph.png}
\caption{Graph representation of a sentence in \textit{SSModel}}\label{fig:SSGraph}
\end{figure}

The representation of a sentence as a graph is shown the fig. \ref{fig:SSGraph}. The sentence "Discrete domains mediate the light-responsive nuclear and cytoplasmic localization of Arabidopsis COP1." is tokenized and every token forms a node in the graph. The node has two rows, first row contains the text of the token and the second row has its part of the speech(POS) tag and the token number joined by a hyphen. The tokens that are part of a protein entity have a green background, the tokens that are part of a location entity have a magenta background and the tokens that are part of organism entity have a yellow  background. As shown in the graph, \textit{COP1} is a protein entity, \textit{Arabidopsis} is an organism name and \textit{nuclear} and \textit{cytoplasmic} are the locations mentioned in the sentence. The nodes in the graph are connected by edges which are the dependency
 relations between the tokens. The type of the edges are represented using colored boxes. For example, there is a dependency edge of the type \textit{amod} from the token \textit{domains}  to  the token \textit{Discrete}. Since the dependency relations between the tokens are directed, this graph representation is a directed graph. 

Also shown are the dashed edges which represents the protein-location relations. Obviously, these dashed edges are just shown for representation and do not form the edges in the actual graph.

\subsection{Graph representation for \textit{DSModel}}\label{sec:graphDSModel}

%TODO add figure for multi sentences

For \textit{DSModel}, the graph representation is bit different compared with that in \textit{SSModel}. In \textit{DSModel}, pair of sentences are considered where every pair consists of neighboring sentences. Figure \ref{fig:DSGraph} shows an example of \textit{DSModel} graph representation. Similar to the graph representation in \textit{SSModel}, the sentence is tokenized and every token forms a node in the graph. The tokens that are part of protein entity are in green, those part of organism entity are in yellow and the tokens that are part of a location entity are in magenta. Since two sentences are considered at a time, the graphs of these sentences are concatenated together and considered as a combined sentence as shown in the fig. \ref{fig:DSGraph}. 

\begin{figure}
\centering
\includegraphics[scale=0.3]{figures/DiffSentenceGraph.png}
\caption{Graph representation of a combined sentence in \textit{DSModel}}\label{fig:DSGraph}
\end{figure}


The edges in the graph are primarily dependency edges between the tokens in two sentences. In addition to the dependency edges, the graph is augmented with several other edges. They are described as follows:

\begin{itemize}

\item \textbf{Root to root connection}: the root of the first sentence is connected with root of the second sentence. This connection helps in finding the path from a entity in one sentence to an entity in another sentence. This connection is bidirectional. As shown in the fig. \ref{fig:DSGraph}, the root of the first sentence "in" is connected via a dependency edge to the root of the second sentence "domains". The forward dependency edge between the roots is labeled \textit{rootDepFwd} and the backward dependency edge between the roots is labeled \textit{rootDepBkwd}.

\item \textbf{Connections between same words}: If some of the tokens in both sentences are nouns and have similar text, then these tokens are connected by edges. These edges are directed from tokens in the first sentence to the tokens in the second sentence.

\item \textbf{Connections between protein entity and "protein" word}: These are special connections which try to gauge the coreferent mentions in the most naive manner. If one of the sentence contains a word "protein" and does not contain any other protein entity, then it is likely that the "protein" word in this sentence may actually be referring to the protein entities in another sentence. Therefore, a connection is made from the word "protein" to the protein entities in other sentences with the expectation that if this is a case of coreferent mentions, then it will be easier to extract the path. 

\item \textbf{Connections between location entity and "location" word}: With the similar assumption as mentioned in the previous point, it is assumed that if a sentence has a word "localize", "location" or "compartment" and it does not have a location entity, then it might be the case that this word is a coreferent mention of the location entity mentioned in the other sentence. Therefore, an extra edge is added between the words containing text "location", "localize" or "compartment" to the location entity. This is a directional edge from the word to the entity. In the fig. \ref{fig:DSGraph}, although there is a word \textit{compartment} in the first sentence, the connections are not shown in the figure to keep the figure readable.

\end{itemize}

Figure \ref{fig:DSGraph} is a representational figure. In the figure, the entity \textit{Stp22p} present in the first sentence has a PL relationship with location entities \textit{vacuole} and \textit{vacuolar} present in the second sentence. These edges are denoted by dashed lines. As mentioned previously, the dashed edges are just for the purpose of illustration and do not form true edges in the graph used for feature extraction. In this way, \textit{DSModel} considers the combination of two neighboring sentences as a single sentence unit and extracts the features from such a combined sentence.

\section{Data Structures} \label{sec:dataStructure}


\section{Feature extraction}\label{sec:featExp}

This section describes the features extracted and used for the purpose of training and classification. The format of the feature set to be written for $SVM^{light}$ is explained in detail in the section \ref{sec:training}. Although there are a few differences in the feature sets of \textit{SSModel} and \textit{DSModel}, most of the features are common to both the models. 

There are three feature subspaces from which the features can be extracted. These feature subspaces correspond to the sequences, syntactic parse trees and dependency parse trees \cite{jiang2007systematic}. A variety of features are extracted from all these feature subspaces and combined for optimal performance. The overview of features can be presented as follows.

\subsection*{Sentence features}

Sentence features are extracted for every potential relation that is a part of the sentence. Sentence features include bag of words (BOW) of sentence tokens, stem of sentence tokens and frequency features like number of protein entities in the sentence, number of location entities in the sentence, number of organism entities in the sentence, count of the bag of words, etc.

\subsection*{Token features}

Token features are extracted for tokens that are part of the entities and for tokens that are in linear and dependency context of entities. Every entity is represented by a head token and the head token of an entity is calculated by a simple heuristic.

Token features include token text, masked token text, stem, POS tag, etc. Most of the features are binary and some are integer-valued features. There are features for binary tests on a token like whether first letter of token is in capital case or not (capitalization test), if some letter in the middle is capital case or not. Other binary tests include presence of a hyphen, presence of forward slash, presence of backward slash, , presence of digits, etc. Character bigrams and trigrams of the token also contribute to the token features.

\subsection*{Linear context and dependency chain}

Token features are also extracted for tokens that are present in the linear and dependency context. A linear context of length 3 is considered, i.e., features are extracted for next 3 tokens and previous 3 tokens of the token under consideration.

A dependency chain length of 3 is considered for the dependency context. Incoming and outgoing dependencies are calculated for tokens. For example, in the case of incoming dependencies, features are extracted for the 'source' token (or 'from' token) and its incoming, outgoing dependencies are considered too. This goes on upto dependency depth of 3. In addition to the token features, the dependency edge types are also considered while extracting dependency chain features.

\subsection*{Dependency features}

\begin{figure}
\centering
\includegraphics[scale=0.3]{figures/ShortestPath.png}
\caption{Shortest Path from protein entity \textit{COP1} to location entity \textit{cytoplasmic}}\label{fig:shortPath}
\end{figure}

A lot of features are extracted depending on the dependency graph. Using Floyd-Warshall algorithm, shortest path between the protein entity and the location entity in a potential PL relation is calculated. For the purpose of extracting shortest path, an undirected graph of dependencies is considered. Figure \ref{fig:shortPath} shows the shortest path from protein entity \textit{COP1} to location entity \textit{cytoplasmic}. The path is shown in bold. Note that undirected graph is considered only for the purpose of extracting shortest path.  Most of the dependency based features depend on this shortest path. While extracting features, the original direction of edges in the shortest path is also taken into consideration. The length of the path contributes an integer valued feature in addition to binary feature for each length.

Token features are extracted for terminal features of the shortest path which are head tokens of the entities. Some of the other path related features include token features of the internal tokens in the path, features for every edge in the path, features for internal edges in the path, etc.

\subsubsection*{N-gram dependency features}

The protein and location entities are represented by their respective head tokens.  The shortest path between two entities is actually a shortest path between the head tokens. However, there need not be a single shortest path between two entities. There can be multiple paths between two entities with same distance or same minimum distances. All such paths with minimum distance are calculated and features are extracted for each one of them.

For every such minimum distance path from the set of min distance paths, parts of the paths are considered for N-gram features. For example, set of all 2 consecutive tokens are considered for 2-gram features and set of all 3 consecutive tokens are considered for 3-gram features, etc. 2,3 and 4-gram features are extracted from all such paths and these features include the token features that are part of the corresponding set, dependencies in the set, directions of dependencies in the set, etc. 

\subsection*{Other features}

Other features include relative order of the entities, the features of the tokens that lie between entities in potential relation, features depending on the presence of word "protein", presence of other special words between two entities, etc.

\subsection{Features specific to \textit{DSModel}}

Some features are very specific to the \textit{DSModel} since it involves processing pair of sentence as a combined sentence along with extra links. Some of those features include presence of an entity in first sentence or second sentence, bag of words/stem/POS of tokens in individual sentences, etc. Importantly, the \textit{DSModel} also makes use of predictions of \textit{SSModel} and related features include if the entities considered in the potential relations have a same sentence relation or not according to \textit{SSModel} predictions. The intuition behind using same sentence predictions is that the entities that already have a same sentence relations is unlikely to have a different sentence relations in most of the cases.

\section{Feature Selection}\label{sec:featSel}


Feature selection is the process of selecting appropriate features from a large set of possible available features. There are several motivations for feature selection, one of which is to avoid overfitting and improve generalization accuracy. Therefore, the task of feature selection can also be described as the task of removing irrelevant features.

%Write some of the results found out from in Joachim's paper [Text categorization with SVM]
%
%**This is a very important property that has got to be known that in text categorization using SVM, there are only few irrelevant features.
%
%When ranked according to Binary information gain, see Figure 1, removal of high ranking features does not take away a lot of information. This also means that even features ranked lowest still contain considerable information and are somewhat relevant.
%

As observed by authors in \cite{joachims1998text}, for the task of text categorization, there are very few irrelevant features. The authors calculated binary information gain for all the features and found out that even the features that are ranked lowest still contain considerable information and are somewhat relevant. In the task of text categorization, the documents are classified into one of the several predefined categories depending on the words present in that document. Therefore, the words/terms present in the document are the features for the task of text categorization and it is easier to discard the words/terms if that particular feature is found irrelevant. 

However, for my task of relation extraction, the features are very complex and discarding individual features is not an option. The features are extracted with the purpose of extracting some characteristic (e.g. binary test of capitalization in the case of token features) and the entire feature generating mechanism (e.g.  feature generating function in our case) can be commented if those features are found ineffective.

Several approaches for the task of feature selection are used and they are explained as below:

\subsection{Leave one out feature generator}

For the task of protein-location relation extraction, the feature generators are the functions that extract certain characteristics. Such feature generators are commented in a leave one out fashion such that the features extracted by that function are discarded and other features are kept as it is. The performance of the method is then calculated. This process is repeated for all feature generators to find out which feature generator produces irrelevant features. The feature generators, which upon commenting boosts the performance of the model, can be then removed from the final set of feature generators.

This approach results in some performance improvement but the biggest drawback of this approach is the independence assumption. This approach assumes that the features $\left\lbrace {fg_1}_i \right\rbrace$ generated by a feature generator $fg_1$ is independent of the features $\left\lbrace {fg_2}_i \right\rbrace$ generated by feature generator $fg_2$. However, these features in set $\left\lbrace {fg_1}_i \right\rbrace$ and $\left\lbrace {fg_2}_i \right\rbrace$ are not strictly independent. Therefore, it may happen that commenting either $\left\lbrace {fg_1}_i \right\rbrace$ or $\left\lbrace {fg_2}_i \right\rbrace$ may not improve any performance but commenting both can. The leave one out feature generator approach does not care about combining the feature sets of individual feature generators and therefore, it does not results in the optimal selection of feature subset.

\subsection{Using information gain}

One of the widely used feature selection technique is to calculate the information gain of the features \cite{yang1997comparative}. The information gain is used to calculate the goodness of the features. The features with higher goodness can be kept and others can be discarded. The information gain for the feature $f$ can be calculated as:

\begin{align}
\begin{aligned}
 G \left( f \right) = & -  \sum^m_{i=1} P_r(c_i) \text{ log } P_r(c_i) \\ 
  & + P_r(f) \sum^m_{i=1} P_r(c_i|f) \text{ log } P_r(c_i|f)\\ 
  & +  P_r(\overline{f}) \sum^m_{i=1} P_r(c_i|\overline{f}) \text{ log } P_r(c_i|\overline{f})  
\end{aligned}  
\end{align}

where $c_i$ is the set of target relations which is $\left\lbrace +1, -1 \right\rbrace$. $f$ indicates the presence of the feature and $\overline{f}$ indicates the absence of it.

\subsection{Using feature weights}

One of the other relevant approach used for feature selection is normal-based feature selection \cite{brank2002feature}. As discussed in the section \ref{sec:SVM}, the support vector machine predictor uses a hyperplane to separate the positive instances from the negative ones. For a linear kernel, the prediction for certain example $x$ can be written as 

$$
prediction(x) = sgn \left[ b + \mathbf{w}^T\mathbf{x} \right]
$$

where $\mathbf{w}$ is the vector of weights. Geometrically, $\mathbf{w}$ is the normal to the hyperplane that is used by the predictor and \cite{brank2002feature} shows that the weights of individual features individually influences the hyperplane and margin. Therefore, the most significant features are the ones which have higher feature weights.

This approach can be used to extract highest weighted features and discard others.


\section{Training, Cross validation and Classification}\label{sec:training}

All experiments are performed on LocText \cite{loctext} corpus.

\subsection{Representation of examples in $SVM^{light}$ }

$SVM^{light}$ \cite{joachims1999making} have a specific way of representing the examples. For protein-location relation extraction, every potential relation is an example. Every example is written on a new line in a plain file and represented as a vector of features. Following format explains the structure of every example:

\bigskip

\texttt{<line> .=. <target> <feature>:<value>...<feature>:<value> \# <info>}

\texttt{<target> .=. +1 | -1 | 0 | <float>}

\texttt{<feature> .=. <integer> | "qid"}

\texttt{<value> .=. <float>}

\texttt{<info> .=. <string>}

\subsection{Cross validation}

\begin{figure}
\centering
\includegraphics[scale=0.6]{figures/CrossValidation.png}
\caption{5-fold cross validation}\label{fig:crossVal}
\end{figure}

A 5-fold cross validation is used where every fold consists of 20 documents. In every round, 60\% of the total data is used for training, 20\% for development and 20\% for test. The training data is used to train the model using $SVM^{light}$. The development data is used for tuning the hyperparameters. For both \textit{SSModel} and \textit{DSModel}, regularization parameter $\mathbf{C}$ is the only hyperparameter to be tuned.  Finally, unseen test data is used for calculating results. The final performance of the model is the average of the performance for all 5 rounds. Figure \ref{fig:crossVal} illustrates the process of cross validation.

%TODO Add more details about other experimentation
\section{Additional experimentation}\label{sec:experiments}

A lot of experiments were designed and their effect was observed on the performance of the models. All of these experiments were based on some or the other intuition. This section describes the effects of all such experiments. This section initially describes the experimentation common to \textit{SSModel} and \textit{DSModel}. The experiments specific to \textit{SSModel} and \textit{DSModel} are discussed later as a subsection.

\subsubsection*{Experimentation with different kernels}

$SVM_{light}$ provides an ability to use several kernels for the purpose of training a model. The default kernel is the linear kernel and other kernels are polynomial, RBF (Radial Basis Function) and sigmoid kernels. Using kernels other than default kernel reduces the performance of the method. Even \cite{joachims1998text} shows that the linear kernel is the best kernel that can be used for text processing.

\subsection{Experimentation specific to \textit{SSModel}}
%TODO

\subsection{Experimentation specific to \textit{DSModel}}

\subsubsection*{Using additional information}

Efforts were made to increase the performance of \textit{DSModel} using additional information extracted from the document. One instance of such additional information is shortform-longform pairs extracted from the document. The intuition was that if either of shortform or longform of a protein is predicted to be in PL relationship with a location entity, then the corresponding longform or shortform should also be in a PL relationship with that entity. Shortform and longform pairs were extracted using a simple algorithm \cite{hearst2003simple}.

However, adding such information actually decreased the performance of \textit{DSModel} since more number of false positives were added than true positives. It was decided, therefore, not to use this augmentation of information.


\section{Tools used}\label{sec:tools}

Stanford corenlp pipeline \cite{manning2014stanford} is used for the process of sentence segmentation, tokenization, syntactic parsing, dependency parsing and coreference resolution. The support vector machine library $SVM^{light}$ \cite{joachims1999making} is used for training and classification.